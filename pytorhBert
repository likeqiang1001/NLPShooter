#!/usr/bin/env python
# coding: utf-8

# ## 引入包

# In[1]:


import torch
from torch import nn
from torch.utils.data import DataLoader,Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor


# 
# ## 加载预训练模型

# In[2]:


from transformers import BertTokenizer,BertForSequenceClassification,BertConfig
config=BertConfig.from_pretrained("D:\\jpdir\\bert\\bertchinese",num_labels=10)
tokenizer = BertTokenizer.from_pretrained("D:\\jpdir\\bert\\bertchinese")
model = BertForSequenceClassification.from_pretrained("D:\\jpdir\\bert\\bertchinese",config=config)


# ## 加载数据文件

# In[3]:


# 整理训练数据
x_train=[]
x_test=[]
with open("D:\\jpdir\\bert\\bertdata\\Multi-classification\\train.txt","r",encoding="utf-8") as f:
    lines=f.readlines()
    for line in lines:
        x_train.append(line.split("\t")[0])
        x_test.append(line.split("\t")[1].replace("\n",""))
        
# 整理测试数据
y_train=[]
y_test=[]
with open("D:\\jpdir\\bert\\bertdata\\Multi-classification\\test.txt","r",encoding="utf-8") as f:
    lines=f.readlines()
    for line in lines:
        y_train.append(line.split("\t")[0])
        y_test.append(line.split("\t")[1].replace("\n",""))


# ## 定义数据

# In[4]:


class CustomDataset(Dataset):
    def __init__(self,data_path):
        # 初始化数据集的过程，例如加载数据等
        # 假设我们有一个数据列表
        self.data = []
        with open(data_path,"r",encoding="utf-8") as f:
            lines=f.readlines()
            for line in lines:
                self.data.append(line)
 
    def __len__(self):
        # 返回数据集的长度
        return len(self.data)
 
    def __getitem__(self, index):
        # 根据索引获取一个样本
        line=self.data[index]
        content=line.split("\t")[0]
        label=line.split("\t")[1].replace("\n","").replace("\"","")
        return content,label


# ## 实例化数据集

# In[5]:


train_data= CustomDataset("D:\\jpdir\\bert\\bertdata\\Multi-classification\\train.txt")
test_data= CustomDataset("D:\\jpdir\\bert\\bertdata\\Multi-classification\\test.txt")
len(train_data),len(test_data)


# ## 使用loader加载数据

# ### 设定最大句子长度

# In[6]:


maxlenhth=32


# ### 定义加padding的函数
#     不够maxlength，就加pad，这的pad对应的索引是0

# In[7]:


def add_padding(data):
    if len(data)<maxlenhth:
        for x in torch.arange(maxlenhth-len(data)):
            data.append(0)
    return data


# ### 定义加collate_fn函数
# 
# 这里处理tokenizer和paading

# In[8]:


def collate_fn(batchData,tokenizer):
    scentence=[line[0] for line in batchData]
    label=[int(line[1]) for line in batchData]

    scentence=torch.tensor([add_padding(tokenizer.encode(one,max_length=32,add_special_tokens=True)) for one in scentence])
    label=torch.tensor(label)
    
    return scentence,label


# ### 使用DataLoader加载数据

# In[9]:


loader = DataLoader(train_data, 5, shuffle=True,collate_fn=lambda x:collate_fn(x,tokenizer))
data_iter = iter(loader)
print(len(data_iter))

# 看下数据
data = next(data_iter)
"长度：",len(data[0]),"data[0]:",data[0],"data[1]:",data[1],"data:",data,data[0].size(),data[1].unsqueeze(1).size()


# ## 定义模型

# ### 测试预训练模型输出
#     BertForSequenceClassification的输入input_ids size是[batch_size,maxlength],labels的size是[batch_size,1]
#     input_ids 是中文转成设定的数字
#     lables是数据的分类标签
# 
# 
# ### 测试预训练模型输出
#     loss 损失值
#     logits 概率分布

# In[10]:


input_ids = torch.tensor(tokenizer.encode("词汇阅读是关键 08年考研暑期英语复习全指南",max_length=32,add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
print(outputs)
loss, logits = outputs
loss, logits


# ### 定义自己的模型

# In[11]:


# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model1 = NeuralNetwork()
print(model1)


# In[23]:


optimizer = torch.optim.AdamW(model.parameters(),lr=0.001)
criterion = torch.nn.CrossEntropyLoss()


# In[37]:


model.train()

for i,batch in enumerate(loader):
    optimizer.zero_grad()
        
    scentenses,labels=batch
    output=model(scentenses,labels=labels.unsqueeze(1))
    loss,logits=output
    
    loss.backward()
    optimizer.step()
    
    print(i,loss.item())


# ## 参考
# https://blog.51cto.com/u_15127680/3841198
